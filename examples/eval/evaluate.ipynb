{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "This notebook is meant to serve as an example for how to evaluate the performance of a trained model, and/or compare to the summation of Q_prime for your inputs. Summed Q` represents using a summation rather than routing and is a good indicator of the baseline performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run imports\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import yaml\n",
    "\n",
    "from ddr._version import __version__\n",
    "from ddr.validation import (\n",
    "    Config,\n",
    "    Metrics,\n",
    "    plot_box_fig,\n",
    "    plot_cdf,\n",
    "    plot_drainage_area_boxplots,\n",
    "    plot_gauge_map,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a config\n",
    "config_path = \"../\"\n",
    "with open(\"./example_config\") as f:\n",
    "    config = Config(**yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading paths to results. We're comparing to summed q_prime as it's a good indicator of if routing is working\n",
    "summed_q_prime_path = Path(\"./summed_q_prime.zarr\")  # To obtain this, please run scripts/summed_q_prime.py,\n",
    "\n",
    "\n",
    "predictions_path = Path(\n",
    "    \"./model_test.zarr\"\n",
    ")  # To obtain this, please run scripts/test.py to evaluate a trained model\n",
    "\n",
    "ds_qp = xr.open_zarr(summed_q_prime_path)\n",
    "ds_pred = xr.open_zarr(predictions_path)\n",
    "ds_qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Metrics\n",
    "results = []\n",
    "\n",
    "ds_qp_ordered = ds_qp.sel(gage_ids=ds_pred.gage_ids.values, time=ds_pred.time.values)\n",
    "results.append(Metrics(pred=ds_qp_ordered.predictions.values, target=ds_qp_ordered.observations.values))\n",
    "results.append(Metrics(pred=ds_pred.predictions.values, target=ds_pred.observations.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to include in the boxplot\n",
    "key_list = [\"bias\", \"rmse\", \"fhv\", \"flv\", \"nse\", \"kge\"]\n",
    "xlabel = [\n",
    "    r\"Bias ($m^3/s$)\",\n",
    "    \"RMSE\",\n",
    "    \"FHV\",\n",
    "    \"FLV\",\n",
    "    \"NSE\",\n",
    "    \"KGE\",\n",
    "]\n",
    "dataset_labels = [\n",
    "    f\"$\\\\sum$ Q` $\\\\delta$HBV2.0UH\",\n",
    "    f\"ddrv{__version__}\",\n",
    "]\n",
    "\n",
    "# Create Box Plots\n",
    "data_box = []\n",
    "for statStr in key_list:\n",
    "    temp = []\n",
    "    for result in results:\n",
    "        data = dict(result)[statStr]\n",
    "        if data.size > 0:  # Check if data is not empty\n",
    "            if statStr == \"nse\" or statStr == \"kge\":\n",
    "                data = np.clip(data, -1, 1)  # Clip NSE and KGE values to [-1, 1]\n",
    "            data = data[~np.isnan(data)]  # Remove NaNs\n",
    "            temp.append(data)\n",
    "    data_box.append(temp)\n",
    "\n",
    "fig = plot_box_fig(\n",
    "    data=data_box,\n",
    "    xlabel_list=xlabel,\n",
    "    legend_labels=dataset_labels,\n",
    "    sharey=False,\n",
    "    figsize=(20, 8),\n",
    "    legend_font_size=18,\n",
    "    xlabel_font_size=14,\n",
    "    tick_font_size=26,\n",
    ")\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "boxPlotName = \"Model Comparison (1995/10/01 - 2010/09/30)\"\n",
    "fig.suptitle(boxPlotName, fontsize=30)\n",
    "plt.rcParams[\"font.size\"] = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a CDF\n",
    "fig, ax = plot_cdf(\n",
    "    data_list=[np.clip(dict(result)[\"nse\"], 0, None) for result in results],\n",
    "    title=\"Model Test Performance (1995/10/01 - 2010/09/30)\",\n",
    "    legend_labels=dataset_labels,\n",
    "    figsize=(16, 8),\n",
    "    xlabel=\"NSE\",\n",
    "    ylabel=\"Cumulative Frequency\",\n",
    "    reference_line=None,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the gauges.csv file, you can run the following function to break up your code's performance by gauge DA\n",
    "gages_df = pd.read_csv(\"training_gauges.csv\")\n",
    "gages_df[\"STAID\"] = gages_df[\"STAID\"].astype(str).str.zfill(8)\n",
    "gages_df = gages_df.set_index(\"STAID\")\n",
    "selected_gages = gages_df.loc[ds_pred.gage_ids.values].reset_index()\n",
    "\n",
    "selected_gages[\"q_prime_NSE\"] = np.clip(results[0].nse, a_min=0.0, a_max=1.0)\n",
    "selected_gages[\"ddr_NSE\"] = np.clip(results[1].nse, a_min=0.0, a_max=1.0)\n",
    "\n",
    "fig = plot_drainage_area_boxplots(\n",
    "    gages=selected_gages, metrics=[\"q_prime_NSE\", \"ddr_NSE\"], model_names=dataset_labels, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gauge_map(\n",
    "    gages=selected_gages,\n",
    "    metric_column=\"ddr_NSE\",\n",
    "    title=f\"ddrv{__version__}\",\n",
    "    show_plot=True,\n",
    "    colormap=\"plasma\",\n",
    "    figsize=(16, 8),\n",
    "    point_size=30,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
